{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20028fd3",
   "metadata": {},
   "source": [
    "### Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f7d0409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first import important libraries\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab971915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connecting with webdriver\n",
    "driver = webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5143b25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening website using webdriver \n",
    "url = ' https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "752be827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets type keywords to search for a job\n",
    "job_search = driver.find_element_by_xpath('//input[@class=\"sugInp\"]')\n",
    "job_search.send_keys('Data Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ba44a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "237a2e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets type the keyword to search for location\n",
    "location_search = driver.find_element_by_xpath('//input[@name=\"location\"]')\n",
    "location_search.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4754fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets click on search\n",
    "search_btn = driver.find_element_by_xpath('//button[@class=\"btn\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d9bcfd",
   "metadata": {},
   "source": [
    "As we have done with opening the webdriver and searching for our desire job and location and clicking on search tab using selenium automation.\n",
    "\n",
    "Now we will scrap our desire data which is mentioned in our question no. 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5a29f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will create 4 empty list for job_title, job_location, company_name and required_experience\n",
    "\n",
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "experience_required = []\n",
    "\n",
    "# lets scrap all the job title first using xpath\n",
    "\n",
    "for i in driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]'):   # getting all the title tags\n",
    "    job_title.append(i.text)                                                    # getting the texts of titles\n",
    "    job_title=job_title[:10]                                                    # as we need first 10 job position only\n",
    "    \n",
    "# lets scrap the locations using xpath\n",
    "\n",
    "for i in driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]//span'): # getting the tags for job location by xpath through parent tag ('li')\n",
    "    job_location.append(i.text)                 # text for job_location\n",
    "    job_location = job_location[:10]            # as we need first 10 job locations only\n",
    "    \n",
    "# lets scrap the company name using xpath\n",
    "\n",
    "for i in driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]'):  # getting tags for company_name\n",
    "    company_name.append(i.text)                 # getting text in list\n",
    "    company_name = company_name[:10]            # as we need first 10 company names only\n",
    "    \n",
    "# lets scrap the last list experience_required using xpath\n",
    "\n",
    "for i in driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]//span'):  # for tags of experience through parent tag(li)\n",
    "    experience_required.append(i.text)                    # getting text in list\n",
    "    experience_required = experience_required[:10]        # as we need first 10 experience_required only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d287a2d9",
   "metadata": {},
   "source": [
    "All lentghs are equal data has been fatched in related lists. Now we are good to go to make DataFrame of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39334461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Flipkart Internet Private Limited</td>\n",
       "      <td>4-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data analysts</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data analysts</td>\n",
       "      <td>Bengaluru/Bangalore</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>5-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Zilingo.com</td>\n",
       "      <td>0-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst with Replicon Software</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Replicon Software (India) Pvt Ltd</td>\n",
       "      <td>2-4 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Analyst / Sr. Data Analyst - SQL</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Naukri Premium - Employer Services</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Publicis Groupe</td>\n",
       "      <td>5-9 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Publicis Groupe</td>\n",
       "      <td>6-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Publicis Groupe</td>\n",
       "      <td>1-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior Associate - Data Analyst</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Sapient</td>\n",
       "      <td>4-6 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Job Title             Location  \\\n",
       "0                    Senior Data Analyst  Bangalore/Bengaluru   \n",
       "1                          Data analysts  Bangalore/Bengaluru   \n",
       "2                          Data analysts  Bengaluru/Bangalore   \n",
       "3                           Data Analyst  Bangalore/Bengaluru   \n",
       "4    Data Analyst with Replicon Software  Bangalore/Bengaluru   \n",
       "5  Data Analyst / Sr. Data Analyst - SQL  Bangalore/Bengaluru   \n",
       "6                    Senior Data Analyst  Bangalore/Bengaluru   \n",
       "7                    Senior Data Analyst  Bangalore/Bengaluru   \n",
       "8                    Senior Data Analyst  Bangalore/Bengaluru   \n",
       "9        Senior Associate - Data Analyst  Bangalore/Bengaluru   \n",
       "\n",
       "                         Company Name Experience Required  \n",
       "0   Flipkart Internet Private Limited             4-5 Yrs  \n",
       "1              IBM India Pvt. Limited             3-5 Yrs  \n",
       "2              IBM India Pvt. Limited             5-6 Yrs  \n",
       "3                         Zilingo.com             0-4 Yrs  \n",
       "4   Replicon Software (India) Pvt Ltd             2-4 Yrs  \n",
       "5  Naukri Premium - Employer Services             2-5 Yrs  \n",
       "6                     Publicis Groupe             5-9 Yrs  \n",
       "7                     Publicis Groupe             6-8 Yrs  \n",
       "8                     Publicis Groupe             1-7 Yrs  \n",
       "9                             Sapient             4-6 Yrs  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a DataFrame for the fetched data\n",
    "jobs = pd.DataFrame({})\n",
    "jobs['Job Title'] = job_title\n",
    "jobs['Location'] = job_location\n",
    "jobs['Company Name'] = company_name\n",
    "jobs['Experience Required'] = experience_required\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a6e76",
   "metadata": {},
   "source": [
    "### Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30cf6cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first connecting with webdriver, opening a chrome window\n",
    "driver = webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c84fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening website using webdriver \n",
    "url = ' https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbd503a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets type keywords to search for a job\n",
    "job_search = driver.find_element_by_xpath('//input[@class=\"sugInp\"]')\n",
    "job_search.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b894bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets type the keyword to search for location\n",
    "location_search = driver.find_element_by_xpath('//input[@name=\"location\"]')\n",
    "location_search.send_keys('Bangalore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f24fb509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets click on search\n",
    "search_btn = driver.find_element_by_xpath('//button[@class=\"btn\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e84d30",
   "metadata": {},
   "source": [
    "As we have done with opening the webdriver and searching for our desire job and location and clicking on search tab using selenium automation.\n",
    "\n",
    "Now we will scrap our desire data which is mentioned in our question no. 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "614af635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will create 4 empty list for job_title, job_location, company_name and job_description\n",
    "\n",
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "\n",
    "\n",
    "# lets scrap all the job title first using xpath\n",
    "\n",
    "for i in driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]'):   # getting all the title tags\n",
    "    job_title.append(i.text)                                                    # getting the texts of titles\n",
    "    job_title=job_title[:10]                                                    # as we need first 10 job position only\n",
    "    \n",
    "# lets scrap the locations using xpath\n",
    "\n",
    "for i in driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]//span'): # getting the tags for job location by xpath through parent tag ('li')\n",
    "    job_location.append(i.text)                 # text for job_location\n",
    "    job_location = job_location[:10]            # as we need first 10 job locations only\n",
    "    \n",
    "# lets scrap the company name using xpath\n",
    "\n",
    "for i in driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]'):  # getting tags for company_name\n",
    "    company_name.append(i.text)                 # getting text in list\n",
    "    company_name = company_name[:10]            # as we need first 10 company names only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e8c661e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets scrap the last list full job_descriptions using xpath\n",
    "job_title_urls = [] # empty list for urls\n",
    "for i in driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]'):  # for tags of job_title\n",
    "    job_title_urls.append(i.get_attribute('href'))                    # getting url in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a16c5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Job descriptionJob descriptionJob Role: Data Scientist/Data Analyst /Business AnalystLocation: Chennai/Bangalore/HyderabadGreetings from CAIA - Center for Artificial Intelligence & Advanced Analytics43% of companies experienced a high deficit of skilled resources with Advanced Analytical skills and AI implementing capabilities in the year 2020. CAIA gives you a great opportunity to enter the world of future technologies and Innovations- Data Science, Analytics, AI, Data Visualization, and Cloud Computing.While 2020 was a year like no other, we are living in an interesting times where data is reshaping the world, and businesses are rapidly adopting technology to gain an edge over others. Hence, there's a substantial increase in demand for technology professionals who can implement systems in data science, machine learning, and AI in Tier 1 and Tier 2 organizations working closely with us.To help you build a sustainable career we would like you to utilize data, software and Analytical approaches in Data Science and AI to upskill and get recruited into an organization appreciating your skilling journey.Applications invited from all Freshers and experienced candidates (0-3 yrs) aspiring to make a career in Artificial Intelligence and Advanced Analytics and Data Science.If you wish to make a shift in your career or undergo a career transition, upskilling is essential since it allows you to learn more about the domain and acquire the required skills.Call to schedule interview Monday -Saturday from 10:00 am to 7 PmKoodesh B- +91 73395 11107Manigandan B - +91 93444 57360Email :careerguidance.koodes@centerforaia.commanigandan@centerforaia.comWhat is needed from you?Freshers who wish to start their career in Analytics and AI and professionals who wish toupskill or change their domain to analytics and emerging technologies are free to apply.Educational background in any one of the following- BE/B.Tech, ME/M Tech, MSc, BSc/MSc Math's and Statistics, B Com, BCA, BSc CS, BSC IT, MSC IT, MCASkills relating to Mathematics/Statistics.Natural passion towards numbers, business, coding, Analytics, and Artificial Intelligence, Machine Learning, visualizationGood verbal and written communication skillsAbility to understand domains in businesses across various sectorsSelection procedure includesAptitude Test & Communication Exam - Online / OfflineSQL/Python test - Online / OfflineCandidates who clear the above will have one-one discussions with our Career Guidance Manager for further evaluation and processing of your Resume.All the Shortlisted candidates will be eligible to continue the corporate training with CAIAWhat you can expect from us?You will get trained on the following modules for a period of 12-14 weeks:SQL & PLSQLData Wrangling using PythonData Visualization Using Power-BIStatistics for Machine LearningArtificial Intelligence, Data InterpretationSupervised & Unsupervised Learning,NLP & Deep LearningCloud Data LakeBusiness intelligence & Data VisualizationSimulation ProjectsExpected Outcome?At the end of the Training you are expected to be well versed with the following:Analysis of large and complex data sets from multiple sourcesDevelopment and evaluation of data analytics models, algorithms, and solutionsUnderstanding/implementation of ML algorithms, performance tuning, and reportingImplementation of algorithms to mine targeted data and the ability to convert data into a business storyTranslation of business requirements into technical requirements; Data extraction, preparation, and transformationIdentification, development, and implementation of statistical techniques and algorithms that address business challenges and adds value to the organizationRequirement Analysis and communication of findings in the form of a meaningful story with the stakeholdersFinding analytical solutions to abstract business issues.Apply objective analysis of facts before coming to a conclusionRoleData AnalystIndustry TypeIT Services & ConsultingFunctional AreaAnalytics & Business IntelligenceEmployment TypeFull Time, PermanentRole CategoryAnalytics & BIEducationUG :B.Tech/BE in Any Specialization, BCA in Any Specialization, B.Sc/B.Sc(Hons) in Any Specialization, Any Graduate, B.Com/B.Com(Hons) in Any Specialization, BBA/BMS/BBM/BBS in Any SpecializationKey SkillsBusiness IntelligencePower BiData WranglingArtificial IntelligenceBig DataITMachine LearningStatisticsAnalyticsDeep LearningBusiness AnalysisSQLData ScienceNLPCloud ComputingData AnalysisData VisualizationData WarehousingData AnalyticsETLPython\",\n",
       " 'Job descriptionRoles and ResponsibilitiesRequirements :- 6-9 years of strong experience in data mining, machine learning and statistical analysis.- BS/ MS/ PhD in Computer Science, Statistics, Applied Math, or related areas from Premier institutes ( only IITs / IISc / BITS / Top NITs or top US university should apply)- Ability to lead and deliver in a fast-paced start-up environment.- Fluency in tools such as Python/ R/ Matlab etc.- Strong intuition for data and Keen aptitude on large scale data analysis- Excellent written and verbal communication skills.- Ability to collaborate across teams and strong interpersonal skills.RoleData AnalystIndustry TypeIT Services & ConsultingFunctional AreaAnalytics & Business IntelligenceEmployment TypeFull Time, PermanentRole CategoryAnalytics & BIEducationUG :B.Sc/B.Sc(Hons) in Computers, StatisticsPG :M.Sc/MS in Computers, StatisticsDoctorate :Ph.D/Doctorate in Statistics, Computers, Other DoctorateKey SkillsData ScienceRData ScientistData MiningStatistical AnalystMachine LearningMATLABPython',\n",
       " 'Job descriptionOur Cause:Rapido is Indias largest bike taxi player focused on solving the first and last mile connectivity problem for India. The primary focus is mobility and changing all facets of mobility across India.We believe that 2 Wheeler are the right mode of transport for developing countries like India and have much more scope than 4 wheelers, which is also reflected in the fact that the number of 2 wheelers is significantly more than the number of 4-wheelers.We have operations in close to 100 cities and are the undisputed market leader in this space. Growing close to 500% year-on-year, we have ambitious targets set for ourselves in the future as well.Role and Responsibilities:Translating business requirements into analytical solutionsPerform data analysis (with a representative sample data slice) and build/prototype the model(s)Work with business users and/or other data scientists to define and close on the model designProvide inputs to the data ingestion/engineering team on input data required by the model, size, format, associations, cleansing requiredIdentify/Provide approach and data to validate the model(s)Collaborate with a technology/data engineering team to transfer the business understanding, get the model productionized and validate the output along with business usersTune the model(s) to improve results provided over timeUnderstand business challenges and goals to formulate the approach for data analysis and model creation that will support business decision makingDo hands-on data analysis and model creationWork closely with data analysts in the team to define and execute on EDA and experiment roadmapWork in highly collaborative teams that strive to build quality systems and provide business valueContinually learn and develop your careerMust have:Passion for understanding business problems and trying to address them by leveraging data - characterized by high-volume, high dimensionality from multiple sourcesAbility to communicate complex models and analysis in a clear and precise mannerExperience with building predictive statistical, behavioral or other models via supervised and unsupervised machine learning, statistical analysis, and other predictive modeling techniques.Experience using R, Python or equivalent statistical/data analysis tools. Ability to transfer that knowledge to different toolsExperience with matrices, distributions and probabilityProficiency with relational databases and NoSQLExperience in Machine Learning, Deep Learning solution, Neural Networks is preferred.Worked in a big data environment before alongside a data engineering team (and data visualization team, data and business analysts)Education and Experience:BE/BS/MS or equivalent in Applied mathematics, statistics, physics, computer science or operations research background is a MUST. Minimum 5-8 years as Data Scientist in real business impact projectsRolesenior data scientistIndustry TypeIT Services & ConsultingFunctional AreaAnalytics & Business IntelligenceEmployment TypeFull Time, PermanentRole CategoryNot mentionedEducationUG :B.Sc/B.Sc(Hons) in Any Specialization, B.Tech/BE in Any SpecializationPG :M.Tech/ME in Any Specialization, M.Sc/MS in Any SpecializationKey SkillsData EngineeringData ModelingQuality SystemsEDAData AnalysisBusiness UnderstandingPythonSkills highlighted with ‘‘ are preferred keyskills',\n",
       " 'Job descriptionRoles Responsibilities :Understand business issues, stakeholder requirement and expectationsPerform relevant modeling analysis and POC exploring multiple techniquesEnsure Analytics model qualityDocument and present findings and recommendations on methodology in a structured way to various stakeholder or partnering teams (Engineering, Tech, Product leader ).Work closely with the Engineering and Tech team to convert those POC into fully scalable products.Active and effective collaboration with other project team members (other Data Scientist, Data Steward, Technology Engineer )Collaboratively manage projects from timeline creation to project completion, managing expectations with leaders and colleaguesEncourage team building, best practices sharing especially with more junior team members.Stay abreast of developments in the area to ensure bringing the most appropriate solutions to our businessAbout YouYou ve built a decent track record in research and data analytics. And you have the communication chops to translate it all into conversations. While you ve worked with global cross-functional teams, you can also put your head down and focus on independent projects. Seeing the big picture takes attention to detail. Keeping up with the fast-changing world of technology takes someone who recognizes that. You know what s happening in big data and you re ready to influence what s next.QualificationsMasters degree in Data Sciences, mathematics or a closely related field3 to 5 years of experience as a Data Scientist in the businessStrong knowledge of statistical modelling, machine learning.Experience in machine learning, supervised and unsupervised: Forecasting, Classification, Data/Text Mining, NLP, Decision Trees, Adaptive Decision Algorithms, Random Forest, Search Algorithms, Neural Networks, Deep Learning AlgorithmsExperience with Optimisation techniques: linear programming, integer programming, genetic algorithm, constrained optimisationExperience in working on distributed or cloud computing platforms such as Google Cloud or Microsoft AzureProficiency in Data Science coding languages like Python, PySpark, SQL, R in a cloud environment like DataBricksExperience using collaborative development tools (git ).Experience working with Agile methodologies (SCRUM)Strong problem solving and excellent communication skills, independent working styleExperience in an FMCG company is a strong plusRoleResearch ScientistIndustry TypeBPO / Call CentreFunctional AreaMedical, Healthcare, R&D, Pharmaceuticals, BiotechnologyEmployment TypeFull Time, PermanentRole CategoryR&DEducationUG :Any GraduatePG :Post Graduation Not RequiredKey SkillsCloud computingGITCodingMachine learningAgileScrumFMCGForecastingSQLPython',\n",
       " 'Job descriptionAs our Senior Data Scientist, youll be an integral player in the Marketplace Data Science team based in Bengaluru. With the latest cutting-edge data science tech at your disposal, youll focus your efforts on bringing our Marketplace systems (i.e. supply, demand, and pricing) to the next level, employing various quantitative techniques such as Machine Learning, Optimization, Simulation, Bayesian Techniques to drive asymmetric values for our businesses at Gojek. Youll be heavily involved in ideation, research, and building prototypes, and the folks in the Data Science Platform will bring your models to production. Your efforts will directly influence the stability and scalability of Gojeks Marketplace stream, and thus to companys top and bottom line as a whole.  What You Will DoWork with Data Scientists, Machine Learning Engineers, and Business Users to build, deploy, and scale Data Science solutions on match-making, supply, pricing problems in Gojek that touch the company s baselineUse your experience in Data Science, Machine Learning, Software Engineering, distributed systems to develop these systems, and work with the platform team to take the systems to productionWork with Business teams to continuously refine and improve the systems to cater to ever-changing Gojek needsDesign and develop world class Data Science solutions to enhance the current stack of Marketplace algorithms for supply, demand, pricingWhat You Will NeedAt least 6 years of experience as a Data Scientist or Machine Learning Engineer, with experience in Python, Golang/Java, and UnixA Bachelors Degree in Computer Science, Statistics, Operations Research, or a relevant quantitative fieldSolid knowledge of Data Science and Machine Learning fundamentals, with proven experience formulating Data Science solutions to business problemsProven ability to recognize business needs and to communicate with multiple stakeholders within the Product Management, Business and Operations teamsExperience in taking Data Science models to productionPrior academic or industry work experience with forecasting methods like auto-regressive models, Markov models, and Kernel-based methodsPrior experience with simulations for modeling the stochastic nature of marketplace supply and demand, and knowledge of the transportation and mobility spaceRoleTeam Lead/Technical LeadIndustry TypeInternetFunctional AreaIT Software - eCommerce, Internet TechnologiesEmployment TypeFull Time, PermanentRole CategoryProgramming & DesignEducationUG :Any GraduatePG :Any PostgraduateKey SkillsUnixProduct managementComputer scienceOperations researchSimulationMachine learningDistribution systemForecastingPythonLogistics',\n",
       " 'Job descriptionAs our Senior Data Scientist, youll be an integral player in the Marketplace Data Science team based in Bengaluru. With the latest cutting-edge data science tech at your disposal, youll focus your efforts on bringing our Marketplace systems (i.e. supply, demand, and pricing) to the next level, employing various quantitative techniques such as Machine Learning, Optimization, Simulation, Bayesian Techniques to drive asymmetric values for our businesses at Gojek. Youll be heavily involved in ideation, research, and building prototypes, and the folks in the Data Science Platform will bring your models to production. Your efforts will directly influence the stability and scalability of Gojeks Marketplace stream, and thus to companys top and bottom line as a whole.  What You Will DoWork with Data Scientists, Machine Learning Engineers, and Business Users to build, deploy, and scale Data Science solutions on match-making, supply, pricing problems in Gojek that touch the company s baselineUse your experience in Data Science, Machine Learning, Software Engineering, distributed systems to develop these systems, and work with the platform team to take the systems to productionWork with Business teams to continuously refine and improve the systems to cater to ever-changing Gojek needsDesign and develop world class Data Science solutions to enhance the current stack of Marketplace algorithms for supply, demand, pricingWhat You Will NeedAt least 6 years of experience as a Data Scientist or Machine Learning Engineer, with experience in Python, Golang/Java, and UnixA Bachelors Degree in Computer Science, Statistics, Operations Research, or a relevant quantitative fieldSolid knowledge of Data Science and Machine Learning fundamentals, with proven experience formulating Data Science solutions to business problemsProven ability to recognize business needs and to communicate with multiple stakeholders within the Product Management, Business and Operations teamsExperience in taking Data Science models to productionPrior academic or industry work experience with forecasting methods like auto-regressive models, Markov models, and Kernel-based methodsPrior experience with simulations for modeling the stochastic nature of marketplace supply and demand, and knowledge of the transportation and mobility spaceRoleTeam Lead/Technical LeadIndustry TypeIT Services & ConsultingFunctional AreaIT Software - eCommerce, Internet TechnologiesEmployment TypeFull Time, PermanentRole CategoryProgramming & DesignEducationUG :Any GraduatePG :Any PostgraduateKey SkillsUnixProduct managementComputer scienceOperations researchSimulationMachine learningDistribution systemForecastingPythonLogistics',\n",
       " 'Job descriptionAs a Sr Data Scientist, you will be responsible for:Working with business stakeholders to understand the business problem/requirements and helping define analytic objectivesBuilding the state of art machine learning or computer vision models according to the industry use case. Develop re-usable components that can be applied to similar problem across industryBeing responsible for development of advanced computer vision solutions for detection of defects in the industrial inspection images (Visual or CT/Xray images).Implementing computer vision technique in image classification, object detection, segmentation, etc. using frameworks like TensorFlow PyTorchIterating and improve the model performances using appropriate transfer learning techniquesBuilding end to end Machine learning or Deep learning pipelines and follow the deployment best practices (Flask, FastAPI, Streamlit, Docker ,etc.)Having Ability to deploy machine or deep learning solutions in to production (Edge or cloud environment)Fuel your passionTo be successful in this role you will:Have a Bachelors / Phd in Computer Science or STEM Majors (Science, Technology, Engineering and Math) with 2+ yrs experience in building computer vision solutions for the industrial use case.Have Minimum 6 yrs of experience with machine learning, computer vision algorithms, and framework to solve object detection, recognition, tracking, segmentation, etc.Have experience on deep learning machine learning libraries Tensorflow, Pytorch, Keras, sklearn, OpenCV, etc.Have Knowledge of Convolutional Networks and Deep Learning algorithmsHave Solid programming skills with Python and/or non-scripting languagesExperience working with NDT inspection (CT/Xray image) data is a plusExperience with Docker containers cloud deployment is plusWork in a way that works for youWe recognize that everyone is different and that the way in which people want to work and deliver at their best is different for everyone too. In this role, we can offer the following flexible working patterns:Working flexible hours - flexing the times when you work in the day to help you fit everything in and work when you are the most productiveRoleDatawarehousing TechnicianIndustry TypePowerFunctional AreaIT Software - Application Programming, MaintenanceEmployment TypeFull Time, PermanentRole CategorySystem Design/Implementation/ERP/CRMEducationUG :Any GraduatePG :Post Graduation Not RequiredKey SkillsComputer scienceComputer visiondeep learningMachine learningCloudNDT inspectionMonitoringAnalyticsNon destructive testingPython',\n",
       " 'Job descriptionAs as Senior Data Scientist, you will be part of cross-disciplinary team, working in concert with partners in Baker Hughes. Potential application areas include Automatic Defect Recognition (ADR) and Remote Monitoring and Diagnostics (RMnD) across NDT (Non Destructive Testing) industrial sectors. The data scientist will lead engagements and develop analytics within defined business objectives to address customer needs and opportunities.As a Sr Data Scientist, you will be responsible for:Working with business stakeholders to understand the business problem/requirements and helping define analytic objectivesBuilding the state of art machine learning or computer vision models according to the industry use case. Develop re-usable components that can be applied to similar problem across industryBeing responsible for development of advanced computer vision solutions for detection of defects in the industrial inspection images (Visual or CT/Xray images).Implementing computer vision technique in image classification, object detection, segmentation, etc. using frameworks like TensorFlow PyTorchIterating and improve the model performances using appropriate transfer learning techniquesBuilding end to end Machine learning or Deep learning pipelines and follow the deployment best practices (Flask, FastAPI, Streamlit, Docker ,etc.)Having Ability to deploy machine or deep learning solutions in to production (Edge or cloud environment)Fuel your passionTo be successful in this role you will:Have a Bachelors / Phd in Computer Science or STEM Majors (Science, Technology, Engineering and Math) with 2+ yrs experience in building computer vision solutions for the industrial use case.Have Minimum 6 yrs of experience with machine learning, computer vision algorithms, and framework to solve object detection, recognition, tracking, segmentation, etc.Have experience on deep learning machine learning libraries Tensorflow, Pytorch, Keras, sklearn, OpenCV, etc.Have Knowledge of Convolutional Networks and Deep Learning algorithmsHave Solid programming skills with Python and/or non-scripting languagesExperience working with NDT inspection (CT/Xray image) data is a plusExperience with Docker containers cloud deployment is plusWork in a way that works for youWe recognize that everyone is different and that the way in which people want to work and deliver at their best is different for everyone too. In this role, we can offer the following flexible working patterns:Working flexible hours - flexing the times when you work in the day to help you fit everything in and work when you are the most productiveWorking with usOur people are at the heart of what we do at Baker Hughes. We know we are better when all of our people are developed, engaged and able to bring their whole authentic selves to work. We invest in the health and well-being of our workforce, train and reward talent and develop leaders at all levels to bring out the best in each other.Working for youOur inventions have revolutionized energy for over a century. But to keep going forward tomorrow, we know we have to push the boundaries today. We prioritize rewarding those who embrace change with a package that reflects how much we value their input. Join us, and you can expect:Contemporary work-life balance policies and wellbeing activitiesComprehensive private medical care optionsSafety net of life insurance and disability programsTailored financial programsAdditional elected or voluntary benefitsRoleDatawarehousing TechnicianIndustry TypeTravel & TourismFunctional AreaIT Software - Application Programming, MaintenanceEmployment TypeFull Time, PermanentRole CategorySystem Design/Implementation/ERP/CRMEducationUG :Any GraduatePG :Post Graduation Not RequiredKey SkillsComputer scienceComputer visiondeep learningMachine learningCloudNDT inspectionMonitoringAnalyticsNon destructive testingPython',\n",
       " 'Job descriptionAs a Sr Data Scientist, you will be responsible for:Working with business stakeholders to understand the business problem/requirements and helping define analytic objectivesBuilding the state of art machine learning or computer vision models according to the industry use case. Develop re-usable components that can be applied to similar problem across industryBeing responsible for development of advanced computer vision solutions for detection of defects in the industrial inspection images (Visual or CT/Xray images).Implementing computer vision technique in image classification, object detection, segmentation, etc. using frameworks like TensorFlow & PyTorchIterating and improve the model performances using appropriate transfer learning techniquesBuilding end to end Machine learning or Deep learning pipelines and follow the deployment best practices (Flask, FastAPI, Streamlit, Docker ,etc.)Having Ability to deploy machine or deep learning solutions in to production (Edge or cloud environment)Fuel your passionTo be successful in this role you will:Have a Bachelors / Phd in Computer Science or STEM Majors (Science, Technology, Engineering and Math) with 2+ yrs experience in building computer vision solutions for the industrial use case.Have Minimum 6 yrs of experience with machine learning, computer vision algorithms, and framework to solve object detection, recognition, tracking, segmentation, etc.Have experience on deep learning & machine learning libraries Tensorflow, Pytorch, Keras, sklearn, OpenCV, etc.Have Knowledge of Convolutional Networks and Deep Learning algorithmsHave Solid programming skills with Python and/or non-scripting languagesExperience working with NDT inspection (CT/Xray image) data is a plusExperience with Docker containers & cloud deployment is plusRoleDatawarehousing TechnicianIndustry TypePowerFunctional AreaIT Software - Application Programming, MaintenanceEmployment TypeFull Time, PermanentRole CategorySystem Design/Implementation/ERP/CRMEducationUG :B.Tech/BE in ComputersPG :Post Graduation Not RequiredKey SkillsComputer scienceComputer visiondeep learningMachine learningCloudNDT inspectionMonitoringAnalyticsNon destructive testingPython',\n",
       " 'Job descriptionThe ChallengeBe a thought leader, partner with your cross-functional partners to foster a data driven payments organization.With a focus on Payments compliance and keeping our platform safe, anticipate and detect new fraud trends and regulatory requirements.Define and evaluate key metrics. Build the data foundation you need, in partnership with the data engineering team.Deliver actionable user-insights to build the best products and modelsDesign experiments to measure the impact of new payments featuresEmpower the product and engineering teams to make data-driven decisionsWhat you need to succeed4+ years industry experience in a quantitative analysis role. Experience in Payments a strong plusFluent in SQL and proficiency in analytical tools such as Python, R, etc.Background in statistics and experience with experimentationSolid understanding of product analyticsExperience or willingness to learn tools to create data pipelines using Airflow/MinervaAbility to communicate clearly and effectively to cross functional partners ofvarying technical levelsRoleSoftware DeveloperIndustry TypeInternetFunctional AreaIT Software - Application Programming, MaintenanceEmployment TypeFull Time, PermanentRole CategoryProgramming & DesignEducationUG :B.Sc/B.Sc(Hons) in Statistics, Any GraduateKey SkillsAirflowQuantitative AnalysisMinervaRData EngineeringXMLBuildProduct AnalyticsStatisticsAnalyticsSQLPython']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we will scrap the job description for each url one by one using selenium\n",
    "job_description = []\n",
    "for i in job_title_urls:\n",
    "    driver.get(i)\n",
    "    try:\n",
    "        description_tag = driver.find_element_by_xpath('//section[@class=\"job-desc\"]')\n",
    "        job_description.append(description_tag.text.replace('\\n',''))\n",
    "    except:\n",
    "        pass\n",
    "job_description = job_description[:10]\n",
    "job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bddd98b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Job Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist / Data Analyst -Business Analyst</td>\n",
       "      <td>Hyderabad/Secunderabad, Chennai, Bangalore/Ben...</td>\n",
       "      <td>Inflexion Analytix Private Limited</td>\n",
       "      <td>Job descriptionJob descriptionJob Role: Data S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lead Data Scientist - Machine Learning/ Data M...</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Wrackle Technologies Pvt Ltd</td>\n",
       "      <td>Job descriptionRoles and ResponsibilitiesRequi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Roppen Transportation Services Private Limited</td>\n",
       "      <td>Job descriptionOur Cause:Rapido is Indias larg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sr Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>NielsenIQ</td>\n",
       "      <td>Job descriptionRoles Responsibilities :Underst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Gojek Tech</td>\n",
       "      <td>Job descriptionAs our Senior Data Scientist, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>GO-JEK India</td>\n",
       "      <td>Job descriptionAs our Senior Data Scientist, y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Baker Hughes - The Network</td>\n",
       "      <td>Job descriptionAs a Sr Data Scientist, you wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>FabHotel Aay Kay Model Town</td>\n",
       "      <td>Job descriptionAs as Senior Data Scientist, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>Baker Hughes Incorporated</td>\n",
       "      <td>Job descriptionAs a Sr Data Scientist, you wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sr Data Scientist, Payments</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>AIRBNB GLOBAL CAPABILITY CENTER PRIVATE LIMITED</td>\n",
       "      <td>Job descriptionThe ChallengeBe a thought leade...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0    Data Scientist / Data Analyst -Business Analyst   \n",
       "1  Lead Data Scientist - Machine Learning/ Data M...   \n",
       "2                              Senior Data Scientist   \n",
       "3                                  Sr Data Scientist   \n",
       "4                              Senior Data Scientist   \n",
       "5                              Senior Data Scientist   \n",
       "6                              Senior Data Scientist   \n",
       "7                              Senior Data Scientist   \n",
       "8                              Senior Data Scientist   \n",
       "9                        Sr Data Scientist, Payments   \n",
       "\n",
       "                                        Job Location  \\\n",
       "0  Hyderabad/Secunderabad, Chennai, Bangalore/Ben...   \n",
       "1                                Bangalore/Bengaluru   \n",
       "2                                Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4                                Bangalore/Bengaluru   \n",
       "5                                Bangalore/Bengaluru   \n",
       "6                                Bangalore/Bengaluru   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                Bangalore/Bengaluru   \n",
       "9                                Bangalore/Bengaluru   \n",
       "\n",
       "                                      Company Name  \\\n",
       "0               Inflexion Analytix Private Limited   \n",
       "1                     Wrackle Technologies Pvt Ltd   \n",
       "2   Roppen Transportation Services Private Limited   \n",
       "3                                        NielsenIQ   \n",
       "4                                       Gojek Tech   \n",
       "5                                     GO-JEK India   \n",
       "6                       Baker Hughes - The Network   \n",
       "7                      FabHotel Aay Kay Model Town   \n",
       "8                        Baker Hughes Incorporated   \n",
       "9  AIRBNB GLOBAL CAPABILITY CENTER PRIVATE LIMITED   \n",
       "\n",
       "                                     Job Description  \n",
       "0  Job descriptionJob descriptionJob Role: Data S...  \n",
       "1  Job descriptionRoles and ResponsibilitiesRequi...  \n",
       "2  Job descriptionOur Cause:Rapido is Indias larg...  \n",
       "3  Job descriptionRoles Responsibilities :Underst...  \n",
       "4  Job descriptionAs our Senior Data Scientist, y...  \n",
       "5  Job descriptionAs our Senior Data Scientist, y...  \n",
       "6  Job descriptionAs a Sr Data Scientist, you wil...  \n",
       "7  Job descriptionAs as Senior Data Scientist, yo...  \n",
       "8  Job descriptionAs a Sr Data Scientist, you wil...  \n",
       "9  Job descriptionThe ChallengeBe a thought leade...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a data frame\n",
    "Jobs = pd.DataFrame({})\n",
    "Jobs['Job Title'] = job_title\n",
    "Jobs['Job Location'] = job_location\n",
    "Jobs['Company Name'] = company_name\n",
    "Jobs['Job Description'] = job_description\n",
    "Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aa965b",
   "metadata": {},
   "source": [
    "# Q3: In this question you have to scrape data using the filters available on the webpage of naukri.com\n",
    "\n",
    "#### You have to use the location and salary filter.\n",
    "\n",
    "#### You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "\n",
    "#### You have to scrape the job-title, job-location, company_name,\n",
    "\n",
    "#### experience_required.\n",
    "\n",
    "#### The location filter to be used is “Delhi/NCR”\n",
    "\n",
    "#### The salary filter to be used is “3-6” lakhs\n",
    "\n",
    "#### The task will be done as shown in the below steps:\n",
    "\n",
    "#### 1. first get the webpage https://www.naukri.com/\n",
    "\n",
    "#### 2. Enter “Data Scientist” in “Skill,Designations,Companies” field .\n",
    "\n",
    "#### 3. Then click the search button.\n",
    "\n",
    "#### 4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "\n",
    "#### 4. Then scrape the data for the first 10 jobs results you get.\n",
    "\n",
    "#### 5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "#### Note- All of the above steps have to be done in code. No step is to be done manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a848428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connecting with webdriver \n",
    "driver = webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d19aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.naukri.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67e164af",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_search = driver.find_element_by_xpath('//input[@class=\"sugInp\"]')\n",
    "job_search.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6385c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath('//button[@class=\"btn\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19edda67",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_filter = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[3]/div[2]/div[3]/label/p')\n",
    "location_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e961e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_filter = driver.find_element_by_xpath('/html/body/div[1]/div[3]/div[2]/section[1]/div[2]/div[4]/div[2]/div[2]/label/p')\n",
    "salary_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcd2bc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating 4 empty list for job title, job location, company name, experience required\n",
    "job_title = []\n",
    "job_location = []\n",
    "company_name = []\n",
    "experience_required = []\n",
    "\n",
    "# scraping job_title\n",
    "for i in driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]'):   # getting all the job title tags\n",
    "    job_title.append(i.text)                                                    # appending the text of title tags in empty list\n",
    "    job_title = job_title[:10]                                                  # getting first 10 job title\n",
    "    \n",
    "# scraping job locations\n",
    "for i in driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]//span'):\n",
    "    job_location.append(i.text)\n",
    "    job_location = job_location[:10]\n",
    "    \n",
    "# scraping company name\n",
    "for i in driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]'):\n",
    "    company_name.append(i.text)\n",
    "    company_name = company_name[:10]\n",
    "    \n",
    "# scraping experience required\n",
    "for i in driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]//span'):\n",
    "    experience_required.append(i.text)\n",
    "    experience_required = experience_required[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "afc3e24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Experience Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Only Fresher / Data Scientist / Data Analyst /...</td>\n",
       "      <td>GABA Consultancy services</td>\n",
       "      <td>Noida, Greater Noida, Delhi / NCR</td>\n",
       "      <td>0-0 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>inVentiv International Pharma Services Pvt. Ltd.</td>\n",
       "      <td>Hyderabad/Secunderabad, Gurgaon/Gurugram, Bang...</td>\n",
       "      <td>3-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>AlgoScale Technologies Private Limited</td>\n",
       "      <td>Noida(Sector-62 Noida)</td>\n",
       "      <td>2-6 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist - SAS ,Python, Spark, H20</td>\n",
       "      <td>Optum Global Solutions (India) Private Limited</td>\n",
       "      <td>(WFH during Covid)</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We are hiring- Data Scientist- Noida</td>\n",
       "      <td>RANDSTAD INDIA PVT LTD</td>\n",
       "      <td>Noida</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist/Data Analyst /Business Analyst</td>\n",
       "      <td>Inflexion Analytix Private Limited</td>\n",
       "      <td>Delhi / NCR</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Team Leader Operations/Data Scientist</td>\n",
       "      <td>Optimint Solutions Pvt. Ltd.</td>\n",
       "      <td>(WFH during Covid)</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>UrbanClap</td>\n",
       "      <td>Pune, Delhi / NCR, Mumbai (All Areas)</td>\n",
       "      <td>1-3 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Scientist (Early Joiner)</td>\n",
       "      <td>R Systems International Ltd.</td>\n",
       "      <td>Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>PredictiVu</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0  Only Fresher / Data Scientist / Data Analyst /...   \n",
       "1                              Senior Data Scientist   \n",
       "2                                     Data Scientist   \n",
       "3           Data Scientist - SAS ,Python, Spark, H20   \n",
       "4               We are hiring- Data Scientist- Noida   \n",
       "5      Data Scientist/Data Analyst /Business Analyst   \n",
       "6              Team Leader Operations/Data Scientist   \n",
       "7                                     Data Scientist   \n",
       "8                      Data Scientist (Early Joiner)   \n",
       "9                                     Data Scientist   \n",
       "\n",
       "                                       Company Name  \\\n",
       "0                         GABA Consultancy services   \n",
       "1  inVentiv International Pharma Services Pvt. Ltd.   \n",
       "2            AlgoScale Technologies Private Limited   \n",
       "3    Optum Global Solutions (India) Private Limited   \n",
       "4                            RANDSTAD INDIA PVT LTD   \n",
       "5                Inflexion Analytix Private Limited   \n",
       "6                      Optimint Solutions Pvt. Ltd.   \n",
       "7                                         UrbanClap   \n",
       "8                      R Systems International Ltd.   \n",
       "9                                        PredictiVu   \n",
       "\n",
       "                                        Job Location Experience Required  \n",
       "0                  Noida, Greater Noida, Delhi / NCR             0-0 Yrs  \n",
       "1  Hyderabad/Secunderabad, Gurgaon/Gurugram, Bang...             3-6 Yrs  \n",
       "2                             Noida(Sector-62 Noida)             2-6 Yrs  \n",
       "3                                 (WFH during Covid)             3-8 Yrs  \n",
       "4                                              Noida             3-7 Yrs  \n",
       "5                                        Delhi / NCR             0-3 Yrs  \n",
       "6                                 (WFH during Covid)             2-5 Yrs  \n",
       "7              Pune, Delhi / NCR, Mumbai (All Areas)             1-3 Yrs  \n",
       "8                      Gurgaon/Gurugram, Delhi / NCR             4-8 Yrs  \n",
       "9              Gurgaon/Gurugram, Bangalore/Bengaluru             4-8 Yrs  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making data frame of scraped data\n",
    "jobs = pd.DataFrame({})\n",
    "jobs['Job Title'] = job_title\n",
    "jobs['Company Name'] = company_name\n",
    "jobs['Job Location'] = job_location\n",
    "jobs['Experience Required'] = experience_required\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c8b5e3",
   "metadata": {},
   "source": [
    "# Q4: Write a python program to scrape data for first 10 job results for Data scientist Designation in Noida location. You have to scrape company_name, No. of days ago when job was posted, Rating of the company.\n",
    "\n",
    "#### This task will be done in following steps:\n",
    "\n",
    "#### 1. first get the webpage https://www.glassdoor.co.in/index.htm\n",
    "\n",
    "#### 2. Enter “Data Scientist” in “Job Title,Keyword,Company” field and enter “Noida” in “location” field.\n",
    "\n",
    "#### 3. Then click the search button.\n",
    "\n",
    "#### 4. Then scrape the data for the first 10 jobs results you get in the above shown page.\n",
    "\n",
    "#### 5. Finally create a dataframe of the scraped data.\n",
    "\n",
    "#### Note- All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8ca573ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a new chrome window\n",
    "driver = webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "baf549bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the url of glassdoor\n",
    "url = 'https://www.glassdoor.co.in/index.htm'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bb09d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering \"Data Scientist\" in \"job title, keywords, company\" field\n",
    "job_search = driver.find_element_by_xpath('//input[@data-test=\"search-bar-keyword-input\"]')\n",
    "job_search.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a278c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entering Noida in Location field\n",
    "location_search = driver.find_element_by_xpath('//input[@data-test=\"search-bar-location-input\"]')\n",
    "location_search.send_keys('Noida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "eaa3e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button\n",
    "search_btn = driver.find_element_by_xpath('//button[@class=\"gd-ui-button ml-std col-auto SearchStyles__newSearchButton css-iixdfr\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2ed194c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists \n",
    "company_name = []\n",
    "no_of_days = []\n",
    "rating = []\n",
    "\n",
    "# scraping the company name\n",
    "for i in driver.find_elements_by_xpath('//a[@class=\" css-l2wjgv e1n63ojh0 jobLink\"]//span'): # getting all html tags for company name\n",
    "    company_name.append(i.text)          # appending the company name text in empty list \n",
    "    company_name = company_name[:10]     # sorting first 10 jobs\n",
    "    \n",
    "# scraping no. of days ago job posted\n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"d-flex align-items-end pl-std css-mi55ob\"]'):\n",
    "    no_of_days.append(i.text)\n",
    "    no_of_days = no_of_days[:10]\n",
    "    \n",
    "# scraping company ratings\n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"d-flex flex-column css-x75kgh e1rrn5ka3\"]'):\n",
    "    if driver.find_elements_by_xpath('//span[@class=\"css-19pjha7 e1cjmv6j1\"]') is None:\n",
    "        rating.append('--')\n",
    "    else:\n",
    "        rating.append(i.text)\n",
    "rating = rating[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a8070809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Job Age</th>\n",
       "      <th>Company Ratings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MasterCard</td>\n",
       "      <td>24h</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pixel Vision</td>\n",
       "      <td>7d</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IBM</td>\n",
       "      <td>6d</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Liberin Technologies Private Limited</td>\n",
       "      <td>7d</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Salasar New Age Technologies</td>\n",
       "      <td>30d+</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ericsson</td>\n",
       "      <td>22d</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Newgen Software</td>\n",
       "      <td>7d</td>\n",
       "      <td>3.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Salesforce</td>\n",
       "      <td>18d</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ginger Webs Pvt. Ltd.</td>\n",
       "      <td>30d+</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>United Airlines</td>\n",
       "      <td>17d</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Company Name Job Age Company Ratings\n",
       "0                            MasterCard     24h             4.2\n",
       "1                          Pixel Vision      7d                \n",
       "2                                   IBM      6d             3.9\n",
       "3  Liberin Technologies Private Limited      7d                \n",
       "4          Salasar New Age Technologies    30d+                \n",
       "5                              Ericsson     22d             4.1\n",
       "6                       Newgen Software      7d             3.3\n",
       "7                            Salesforce     18d             4.5\n",
       "8                 Ginger Webs Pvt. Ltd.    30d+                \n",
       "9                       United Airlines     17d             4.1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating Data Frame of the data we scraped\n",
    "jobs = pd.DataFrame({})\n",
    "jobs['Company Name'] = company_name\n",
    "jobs['Job Age'] = no_of_days\n",
    "jobs['Company Ratings'] = rating\n",
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b63edb",
   "metadata": {},
   "source": [
    "# Q5: Write a python program to scrape the salary data for Data Scientist designation in Noida location.\n",
    "\n",
    "#### You have to scrape Company name, Number of salaries, Average salary, Min salary, Max Salary.\n",
    "\n",
    "#### The above task will be, done as shown in the below steps:\n",
    "\n",
    "#### 1. first get the webpage https://www.glassdoor.co.in/Salaries/index.htm\n",
    "\n",
    "#### 2. Enter “Data Scientist” in Job title field and “Noida” in location field.\n",
    "\n",
    "#### 3. Click the search button.\n",
    "\n",
    "#### 4. After that you will land on the below page you have to scrape whole data from this webpage\n",
    "\n",
    "#### 5. Scrape data for first 10 companies. Scrape the min salary, max salary, company \n",
    "\n",
    "#### name, Average salary and rating of the company.\n",
    "\n",
    "#### 6.Store the data in a dataframe.\n",
    "\n",
    "#### Note that all of the above steps have to be done by coding only and not manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "86cf8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a new chrome window\n",
    "driver = webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ec180ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the url for salary data\n",
    "url = 'https://www.glassdoor.co.in/Salaries/index.htm'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "62e6062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_search = driver.find_element_by_xpath('//input[@data-test=\"search-bar-keyword-input\"]')\n",
    "job_search.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bbfe49fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_search = driver.find_element_by_xpath('//input[@data-test=\"search-bar-location-input\"]')\n",
    "location_search.send_keys('Noida')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a4bcc0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn = driver.find_element_by_xpath('//button[@class=\"gd-btn-mkt\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0865e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list for data scientist salary data\n",
    "company_name = []\n",
    "average_salary = []\n",
    "min_salary = []\n",
    "max_salary = []\n",
    "no_of_salary = []\n",
    "ratings = []\n",
    "\n",
    "# scraping the company name\n",
    "for i in driver.find_elements_by_xpath('//a[@class=\"css-f3vw95 e1aj7ssy3\"]'):  # getting html tags for company name\n",
    "    company_name.append(i.text)                                                # appending text(company name) in empty list\n",
    "    company_name = company_name[:10]                                           # sorting first 10 company name\n",
    "    \n",
    "# scraping average salary data\n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"col-12 col-lg-4 px-lg-0 d-flex align-items-baseline\"]//h3'):\n",
    "    average_salary.append(i.text)\n",
    "    average_salary = average_salary[:10]\n",
    "    \n",
    "# scraping min salary\n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"d-flex mt-xxsm css-79elbk epuxyqn0\"]//p[1]'):\n",
    "    min_salary.append(i.text)\n",
    "    min_salary = min_salary[:10]\n",
    "    \n",
    "# scraping max salary\n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"d-flex mt-xxsm css-79elbk epuxyqn0\"]//p[2]'):\n",
    "    max_salary.append(i.text)\n",
    "    max_salary = max_salary[:10]\n",
    "\n",
    "# scraping number of salaries data of each company\n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"col-12 col-lg-auto\"]//span'):\n",
    "    no_of_salary.append(i.text)\n",
    "    no_of_salary = no_of_salary[:10]\n",
    "\n",
    "# scraping rating of the company\n",
    "for i in driver.find_elements_by_xpath('//div[@class=\"d-flex align-items-center mt-xxsm\"]//span[1]'):\n",
    "    ratings.append(i.text)\n",
    "    ratings = ratings[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ffc43e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Average Salary</th>\n",
       "      <th>Minimum Salary</th>\n",
       "      <th>Maximum Salary</th>\n",
       "      <th>Number of Salaries</th>\n",
       "      <th>Ratings of the Company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IBM</td>\n",
       "      <td>₹9,00,000</td>\n",
       "      <td>₹6L</td>\n",
       "      <td>₹27L</td>\n",
       "      <td>18 salaries</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tata Consultancy Services</td>\n",
       "      <td>₹6,15,289</td>\n",
       "      <td>₹3L</td>\n",
       "      <td>₹13L</td>\n",
       "      <td>17 salaries</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accenture</td>\n",
       "      <td>₹11,63,336</td>\n",
       "      <td>₹6L</td>\n",
       "      <td>₹22L</td>\n",
       "      <td>15 salaries</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Delhivery</td>\n",
       "      <td>₹12,18,244</td>\n",
       "      <td>₹5L</td>\n",
       "      <td>₹1Cr</td>\n",
       "      <td>15 salaries</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ericsson-Worldwide</td>\n",
       "      <td>₹7,39,238</td>\n",
       "      <td>₹4L</td>\n",
       "      <td>₹16L</td>\n",
       "      <td>14 salaries</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UnitedHealth Group</td>\n",
       "      <td>₹13,00,000</td>\n",
       "      <td>₹8L</td>\n",
       "      <td>₹15L</td>\n",
       "      <td>13 salaries</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Valiance Solutions</td>\n",
       "      <td>₹8,63,750</td>\n",
       "      <td>₹5L</td>\n",
       "      <td>₹15L</td>\n",
       "      <td>10 salaries</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>EXL Service</td>\n",
       "      <td>₹11,10,000</td>\n",
       "      <td>₹6L</td>\n",
       "      <td>₹15L</td>\n",
       "      <td>9 salaries</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Optum</td>\n",
       "      <td>₹14,23,677</td>\n",
       "      <td>₹8L</td>\n",
       "      <td>₹20L</td>\n",
       "      <td>9 salaries</td>\n",
       "      <td>3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Optum Global Solutions</td>\n",
       "      <td>₹13,28,697</td>\n",
       "      <td>₹4L</td>\n",
       "      <td>₹22L</td>\n",
       "      <td>9 salaries</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Company Name Average Salary Minimum Salary Maximum Salary  \\\n",
       "0                        IBM      ₹9,00,000            ₹6L           ₹27L   \n",
       "1  Tata Consultancy Services      ₹6,15,289            ₹3L           ₹13L   \n",
       "2                  Accenture     ₹11,63,336            ₹6L           ₹22L   \n",
       "3                  Delhivery     ₹12,18,244            ₹5L           ₹1Cr   \n",
       "4         Ericsson-Worldwide      ₹7,39,238            ₹4L           ₹16L   \n",
       "5         UnitedHealth Group     ₹13,00,000            ₹8L           ₹15L   \n",
       "6         Valiance Solutions      ₹8,63,750            ₹5L           ₹15L   \n",
       "7                EXL Service     ₹11,10,000            ₹6L           ₹15L   \n",
       "8                      Optum     ₹14,23,677            ₹8L           ₹20L   \n",
       "9     Optum Global Solutions     ₹13,28,697            ₹4L           ₹22L   \n",
       "\n",
       "  Number of Salaries Ratings of the Company  \n",
       "0        18 salaries                    3.9  \n",
       "1        17 salaries                    3.9  \n",
       "2        15 salaries                    4.1  \n",
       "3        15 salaries                    3.9  \n",
       "4        14 salaries                      4  \n",
       "5        13 salaries                    3.6  \n",
       "6        10 salaries                    4.2  \n",
       "7         9 salaries                    3.6  \n",
       "8         9 salaries                    3.7  \n",
       "9         9 salaries                    3.9  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salaries = pd.DataFrame({})\n",
    "salaries['Company Name'] = company_name\n",
    "salaries['Average Salary'] = average_salary\n",
    "salaries['Minimum Salary'] = min_salary\n",
    "salaries['Maximum Salary'] = max_salary\n",
    "salaries['Number of Salaries'] = no_of_salary\n",
    "salaries['Ratings of the Company'] = ratings\n",
    "salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc0f9a5",
   "metadata": {},
   "source": [
    "# Q6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "#### 1. Brand\n",
    "#### 2. Product Description\n",
    "#### 3. Price\n",
    "#### 4. Discount %\n",
    "#### To scrape the data you have to go through following steps:\n",
    "#### 1. Go to flipkart webpage by url https://www.flipkart.com/\n",
    "#### 2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "#### 3. after that you will reach to a webpage having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "#### 4. after scraping data from the first page, go to the “Next” Button at the bottom of the page , then click on it\n",
    "#### 5. Now scrape data from this page as usual\n",
    "#### 6. repeat this until you get data for 100 sunglasses.\n",
    "#### Note that all of the above steps have to be done by coding only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "498a6309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a new chrome window\n",
    "driver = webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "c852e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening url of related website\n",
    "url = 'https://www.flipkart.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "f1de968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sending keys to product search column\n",
    "product_search = driver.find_element_by_xpath('//input[@title=\"Search for products, brands and more\"]')\n",
    "product_search.send_keys('Sunglasses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "66c0a185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button\n",
    "search_btn = driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "09fb33df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list for the required data\n",
    "brand = []\n",
    "prod_desc = []\n",
    "price = []\n",
    "discount = []\n",
    "\n",
    "# setting the page limit upto which we can get 100 products data\n",
    "start = 0\n",
    "end = 3\n",
    "\n",
    "# scraping data for sunglasses brand\n",
    "for page in range(start,end):  # for loop for 3 pages in range so that each page's data could be scraped iteratively\n",
    "    for i in driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]'): # getting web elements for brand names\n",
    "        brand.append(i.text) # appending brand name data in brand empty list\n",
    "        \n",
    "# scraping data for product desciption.        \n",
    "    for i in driver.find_elements_by_xpath('//a[@class=\"IRpwTa\"]'): # getting web elements of product description\n",
    "        if i.text is None:          # if somewhere text is missing in web element\n",
    "            prod_desc.append('--')  # then append desh so that our length of data could be same and related with other data\n",
    "        else:\n",
    "            prod_desc.append(i.text) # appending text of product description in empty list\n",
    "\n",
    "# scraping data for price data\n",
    "    for i in driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]'):     # getting web elements in which price data is present\n",
    "        price.append(i.text)                                               # appending price data in empty list\n",
    "        \n",
    "# scraping data for discount percentage of the product\n",
    "    for i in driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]//span'): # getting web elements of discount data\n",
    "        if i.text is None:                                      # if somewhere discount is missing apending empty list with desh\n",
    "            discount.append('--')\n",
    "        else:\n",
    "            discount.append(i.text)    # appending empty list with discount data \n",
    "            \n",
    "# getting next page for next page data through next page's web elemnt till above mentioned page range in for loop\n",
    "    nxt_btn = driver.find_elements_by_xpath('//a[@class=\"_1LKTO3\"]') # getting next button web element\n",
    "    try:                                                          \n",
    "        driver.get(nxt_btn[1].get_attribute('href'))   # trying opening next page through the href link present in web element\n",
    "    except:\n",
    "        driver.get(nxt_btn[0].get_attribute('href')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ec2e8a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 120 120 120\n"
     ]
    }
   ],
   "source": [
    "# checking length of each column data so that we make data frame having related value of same product in each row\n",
    "print(len(discount),len(price),len(prod_desc),len(brand))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8f8640a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wrogn</td>\n",
       "      <td>Mirrored Wayfarer Sunglasses (51)</td>\n",
       "      <td>₹663</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>povty</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹342</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹513</td>\n",
       "      <td>35% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (Free Size)</td>\n",
       "      <td>₹733</td>\n",
       "      <td>18% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Gradient, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹509</td>\n",
       "      <td>36% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>AISLIN</td>\n",
       "      <td>UV Protection, Mirrored Wayfarer Sunglasses (61)</td>\n",
       "      <td>₹735</td>\n",
       "      <td>73% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>United Colors of Benetton</td>\n",
       "      <td>Mirrored Rectangular Sunglasses (53)</td>\n",
       "      <td>₹1,574</td>\n",
       "      <td>66% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection, Mirrored Wayfarer Sunglasses (61)</td>\n",
       "      <td>₹788</td>\n",
       "      <td>12% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Wrogn</td>\n",
       "      <td>Mirrored Round Sunglasses (52)</td>\n",
       "      <td>₹708</td>\n",
       "      <td>72% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Shield Sunglasses (Free Size)</td>\n",
       "      <td>₹499</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Brand  \\\n",
       "0                        Wrogn   \n",
       "1                        povty   \n",
       "2                     Fastrack   \n",
       "3                     Fastrack   \n",
       "4                     Fastrack   \n",
       "..                         ...   \n",
       "115                     AISLIN   \n",
       "116  United Colors of Benetton   \n",
       "117                   Fastrack   \n",
       "118                      Wrogn   \n",
       "119             ROZZETTA CRAFT   \n",
       "\n",
       "                                   Product Description   Price Discount  \n",
       "0                    Mirrored Wayfarer Sunglasses (51)    ₹663  73% off  \n",
       "1        UV Protection Wayfarer Sunglasses (Free Size)    ₹342  65% off  \n",
       "2     UV Protection Rectangular Sunglasses (Free Size)    ₹513  35% off  \n",
       "3        UV Protection Wayfarer Sunglasses (Free Size)    ₹733  18% off  \n",
       "4    Gradient, UV Protection Wayfarer Sunglasses (F...    ₹509  36% off  \n",
       "..                                                 ...     ...      ...  \n",
       "115   UV Protection, Mirrored Wayfarer Sunglasses (61)    ₹735  73% off  \n",
       "116               Mirrored Rectangular Sunglasses (53)  ₹1,574  66% off  \n",
       "117   UV Protection, Mirrored Wayfarer Sunglasses (61)    ₹788  12% off  \n",
       "118                     Mirrored Round Sunglasses (52)    ₹708  72% off  \n",
       "119        UV Protection Shield Sunglasses (Free Size)    ₹499  80% off  \n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating data frame\n",
    "data = pd.DataFrame({})\n",
    "data['Brand'] = brand\n",
    "data['Product Description'] = prod_desc\n",
    "data['Price'] = price\n",
    "data['Discount'] = discount\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398b1ab2",
   "metadata": {},
   "source": [
    "# Q7: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes\u0002earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\n",
    "\n",
    "#### 1. Rating \n",
    "#### 2. Review_summary \n",
    "#### 3. Full review\n",
    "#### You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "139bdd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a new chrome window\n",
    "driver = webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "89abc30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the url in the chrome window\n",
    "url = ' https://www.flipkart.com/apple-iphone-11-black-64-gb-includes\u0002earpods-power\u0002adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "ca7b057c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening all reviews page where we can find all pages link\n",
    "all_reviews = driver.find_element_by_xpath('/html/body/div[1]/div/div[3]/div[1]/div[2]/div[8]/div/div/div[5]/div/a')\n",
    "driver.get(all_reviews.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "857d668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "page_urls = []\n",
    "rating_star = []\n",
    "review_summary = []\n",
    "full_review = []\n",
    "\n",
    "# scraping urls of first 10 pages which is available on the same page\n",
    "url_1 = driver.find_element_by_xpath('//a[@class=\"ge-49M _2Kfbh8\"]') # getting first page url in page_urls[]\n",
    "page_urls.append(url_1.get_attribute('href'))\n",
    "\n",
    "url_2 = driver.find_elements_by_xpath('//a[@class=\"ge-49M\"]')  # getting rest of 9 pages urls in page_urls[] list\n",
    "for i in url_2:\n",
    "    page_urls.append(i.get_attribute('href'))\n",
    "    \n",
    "    \n",
    "# now getting all the desire data through each page of reviews iteratively\n",
    "for i in page_urls:\n",
    "    driver.get(i)\n",
    "    \n",
    "    for j in driver.find_elements_by_xpath('//div[@class=\"col _2wzgFH K0kLPL\"]/div[1]/div[1]'): # getting web elements of ratings\n",
    "        rating_star.append(j.text)            # appending rating data into empty list of rating_star[]\n",
    "        \n",
    "    for k in driver.find_elements_by_xpath('//p[@class=\"_2-N8zT\"]'):  # getting web elements of review summary \n",
    "        review_summary.append(k.text)  # appending the review summary text data into empty list of review_summary[]\n",
    "        \n",
    "    for l in driver.find_elements_by_xpath('//div[@class=\"t-ZTKy\"]/div/div'):  # getting web elements of full reviews through parent tag\n",
    "        full_review.append(l.text)  # appending full reviews to the empty list of full_review[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "ad61bcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(rating_star),len(review_summary),len(full_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "6cfc1e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ratings in Star</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Simply awesome</td>\n",
       "      <td>Really satisfied with the Product I received.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>This is my first iOS phone. I am very happy wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Fabulous!</td>\n",
       "      <td>Everything is perfect pictures come out so cle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Excellent</td>\n",
       "      <td>Value for money product. This iphone 11 is rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>4</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>I genuinely liked it. One of the best mobile p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>4</td>\n",
       "      <td>Good quality product</td>\n",
       "      <td>Awesome camera, smooth and fast UI, display is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>Excellent camera 📸 And Display touching very N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Ratings in Star         Review Summary  \\\n",
       "0                5         Simply awesome   \n",
       "1                5              Brilliant   \n",
       "2                5              Fabulous!   \n",
       "3                5       Perfect product!   \n",
       "4                5      Worth every penny   \n",
       "..             ...                    ...   \n",
       "95               5              Fabulous!   \n",
       "96               5              Excellent   \n",
       "97               4              Wonderful   \n",
       "98               4   Good quality product   \n",
       "99               5  Mind-blowing purchase   \n",
       "\n",
       "                                          Full Review  \n",
       "0   Really satisfied with the Product I received.....  \n",
       "1   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "2   This is my first iOS phone. I am very happy wi...  \n",
       "3   Amazing phone with great cameras and better ba...  \n",
       "4   Previously I was using one plus 3t it was a gr...  \n",
       "..                                                ...  \n",
       "95  Everything is perfect pictures come out so cle...  \n",
       "96  Value for money product. This iphone 11 is rea...  \n",
       "97  I genuinely liked it. One of the best mobile p...  \n",
       "98  Awesome camera, smooth and fast UI, display is...  \n",
       "99  Excellent camera 📸 And Display touching very N...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating data frame for scraped data\n",
    "i_phone_data = pd.DataFrame({})\n",
    "i_phone_data['Ratings in Star'] = rating_star\n",
    "i_phone_data['Review Summary'] = review_summary\n",
    "i_phone_data['Full Review'] = full_review\n",
    "i_phone_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b0ca2",
   "metadata": {},
   "source": [
    "# Q8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "#### You have to scrape 4 attributes of each sneaker :\n",
    "#### 1. Brand\n",
    "#### 2. Product Description\n",
    "#### 3. Price\n",
    "#### 4. discount %\n",
    "#### Also note that all the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "b2167fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instatntiating a new chrome window\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "5a970e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the website\n",
    "url = 'https://www.flipkart.com/'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "1c3d2d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing sneakers to search for sneakers in search bar\n",
    "prod_search = driver.find_element_by_xpath('//input[@class=\"_3704LK\"]')\n",
    "prod_search.send_keys('Sneakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "cc57d767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking the search button\n",
    "search_btn = driver.find_element_by_xpath('//button[@class=\"L0Z3Pu\"]')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "4987e94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120 120 120 120\n"
     ]
    }
   ],
   "source": [
    "# creating empty list\n",
    "brand = []\n",
    "prod_desc = []\n",
    "price = []\n",
    "discount = []\n",
    "prod_url = []\n",
    "\n",
    "# setting the page limit upto which we can get 100 products data\n",
    "start = 0\n",
    "end = 3\n",
    "\n",
    "# scraping data for brand\n",
    "for page in range(start,end): # for loop for 3 pages to get data from 3 pages\n",
    "    for i in driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]'):  # getting web elements for brand\n",
    "        brand.append(i.text)# appending text data into empty list through web elements\n",
    "        \n",
    "# scraping data for product description\n",
    "    for i in driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]'): # getting web elements for price\n",
    "        price.append(i.text) # appending the price data into empty list\n",
    "\n",
    "# scraping data for discounts on product\n",
    "    for i in driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]//span'):  # getting web elements for discount data\n",
    "        discount.append(i.text)  # appending the discount data on empty list\n",
    "\n",
    "# scraping product url \n",
    "    for i in driver.find_elements_by_xpath('//a[@class=\"_2UzuFa\"]'):\n",
    "        prod_url.append(i.get_attribute('href'))\n",
    "        \n",
    "# after geting first page data, getting data from next page\n",
    "    nxt_btn = driver.find_elements_by_xpath('//a[@class=\"_1LKTO3\"]')\n",
    "    try:\n",
    "        driver.get(nxt_btn[1].get_attribute('href'))\n",
    "    except:\n",
    "        driver.get(nxt_btn[0].get_attribute('href'))\n",
    "        \n",
    "# scraping product description from each product url iteratively        \n",
    "for i in prod_url:\n",
    "    driver.get(i)\n",
    "    for i in driver.find_elements_by_xpath('//span[@class=\"B_NuCI\"]'):\n",
    "        prod_desc.append(i.text)\n",
    "print(len(prod_desc),len(brand),len(price),len(discount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "9358e912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>price</th>\n",
       "      <th>Percentage of Discount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PUMA</td>\n",
       "      <td>Cape IDP Sneakers For Men  (Black)</td>\n",
       "      <td>₹1,443</td>\n",
       "      <td>58% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADIDAS</td>\n",
       "      <td>Breaknet Sneakers For Men  (White)</td>\n",
       "      <td>₹1,932</td>\n",
       "      <td>57% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ROCKFIELD</td>\n",
       "      <td>Sneakers For Men  (Black)</td>\n",
       "      <td>₹299</td>\n",
       "      <td>70% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>World Wear Footwear</td>\n",
       "      <td>5011-Latest Collection Stylish Casual Loafer S...</td>\n",
       "      <td>₹240</td>\n",
       "      <td>51% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PEHANOSA</td>\n",
       "      <td>Sneakers For Men  (Multicolor)</td>\n",
       "      <td>₹495</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Baogi</td>\n",
       "      <td>Sneakers For Men  (White)</td>\n",
       "      <td>₹449</td>\n",
       "      <td>55% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>ASIAN</td>\n",
       "      <td>Skypy-31 Walking Shoes,Training Shoes,Sneakers...</td>\n",
       "      <td>₹594</td>\n",
       "      <td>25% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Combo Pack of 2 Sports Running LightWeight Sho...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>81% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>WRIZT</td>\n",
       "      <td>Casual Sneakers,dancing, walking Sneakers For ...</td>\n",
       "      <td>₹279</td>\n",
       "      <td>72% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Edoeviv</td>\n",
       "      <td>Original Luxury Branded Fashionable Men's Casu...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>40% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Brand                                Product Description  \\\n",
       "0                   PUMA                 Cape IDP Sneakers For Men  (Black)   \n",
       "1                 ADIDAS                 Breaknet Sneakers For Men  (White)   \n",
       "2              ROCKFIELD                          Sneakers For Men  (Black)   \n",
       "3    World Wear Footwear  5011-Latest Collection Stylish Casual Loafer S...   \n",
       "4               PEHANOSA                     Sneakers For Men  (Multicolor)   \n",
       "..                   ...                                                ...   \n",
       "115                Baogi                          Sneakers For Men  (White)   \n",
       "116                ASIAN  Skypy-31 Walking Shoes,Training Shoes,Sneakers...   \n",
       "117               BRUTON  Combo Pack of 2 Sports Running LightWeight Sho...   \n",
       "118                WRIZT  Casual Sneakers,dancing, walking Sneakers For ...   \n",
       "119              Edoeviv  Original Luxury Branded Fashionable Men's Casu...   \n",
       "\n",
       "      price Percentage of Discount  \n",
       "0    ₹1,443                58% off  \n",
       "1    ₹1,932                57% off  \n",
       "2      ₹299                70% off  \n",
       "3      ₹240                51% off  \n",
       "4      ₹495                50% off  \n",
       "..      ...                    ...  \n",
       "115    ₹449                55% off  \n",
       "116    ₹594                25% off  \n",
       "117    ₹474                81% off  \n",
       "118    ₹279                72% off  \n",
       "119    ₹474                40% off  \n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a data frame of scraped data\n",
    "sneakers_data = pd.DataFrame({})\n",
    "sneakers_data['Brand'] = brand\n",
    "sneakers_data['Product Description'] = prod_desc\n",
    "sneakers_data['price'] = price\n",
    "sneakers_data['Percentage of Discount'] = discount\n",
    "sneakers_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93480c3c",
   "metadata": {},
   "source": [
    "# Q9: Go to the link - https://www.myntra.com/shoes\n",
    "### Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”.\n",
    "### And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoes.\n",
    "### Please note that applying the filter and scraping the data , everything should be done through code only and there should not be any manual step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "5456bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a new chrome window\n",
    "driver = webdriver.Chrome('chromedriver.exe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "e48635d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the url\n",
    "url = 'https://www.myntra.com/shoes'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "9ed78f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on price filter\n",
    "price_filter = driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[5]/ul/li[2]/label/div')\n",
    "price_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "3cdfcd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on color filter\n",
    "color_filter = driver.find_element_by_xpath('/html/body/div[2]/div/div[1]/main/div[3]/div[1]/section/div/div[6]/ul/li[1]/label/div')\n",
    "color_filter.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "3864bd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "# creating empty list\n",
    "brand = []\n",
    "description = []\n",
    "price = []\n",
    "\n",
    "# setting page limit upto which data needed to be scraped\n",
    "start = 0\n",
    "end = 2\n",
    "\n",
    "#scraping data for brand\n",
    "for page in range(start,end):  # for loop for page\n",
    "    for i in driver.find_elements_by_xpath('//h3[@class=\"product-brand\"]'):\n",
    "        brand.append(i.text)\n",
    "        \n",
    "# scraping data for product description\n",
    "    for i in driver.find_elements_by_xpath('//h4[@class=\"product-product\"]'):\n",
    "        description.append(i.text)\n",
    "        \n",
    "# scraping data for price\n",
    "    for i in driver.find_elements_by_xpath('//div[@class=\"product-price\"]'):\n",
    "        price.append(i.text)\n",
    "        \n",
    "# getting data from next page\n",
    "    nxt_btn = driver.find_elements_by_xpath('//li[@class=\"pagination-next\"]//a')\n",
    "    try:\n",
    "        driver.get(nxt_btn[1].get_attribute('href'))\n",
    "    except:\n",
    "        driver.get(nxt_btn[0].get_attribute('href'))\n",
    "print(len(brand),len(description),len(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "7f0a863a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Shoe Brand</th>\n",
       "      <th>Shoe Short Description</th>\n",
       "      <th>Shoe Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men AIR ZOOM Running Shoes</td>\n",
       "      <td>Rs. 7721Rs. 10295(25% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ALDO</td>\n",
       "      <td>Men Driving Shoes</td>\n",
       "      <td>Rs. 11999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nike</td>\n",
       "      <td>LEBRON XVIII Basketball Shoes</td>\n",
       "      <td>Rs. 11436Rs. 17595(35% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men REACT INFINITY RUN Shoes</td>\n",
       "      <td>Rs. 14495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Unisex COSMIC UNITY Basketball</td>\n",
       "      <td>Rs. 13495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Saint G</td>\n",
       "      <td>Women Leather Heeled Boots</td>\n",
       "      <td>Rs. 6460Rs. 7600(15% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Heel &amp; Buckle London</td>\n",
       "      <td>Women Block Heels</td>\n",
       "      <td>Rs. 6392Rs. 7990(20% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Ruosh</td>\n",
       "      <td>Men Leather Monk Shoes</td>\n",
       "      <td>Rs. 5490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Geox</td>\n",
       "      <td>Men Leather Sneakers</td>\n",
       "      <td>Rs. 10999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ASICS</td>\n",
       "      <td>Women Running Shoes</td>\n",
       "      <td>Rs. 6209Rs. 8999(31% OFF)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Shoe Brand          Shoe Short Description  \\\n",
       "0                   Nike      Men AIR ZOOM Running Shoes   \n",
       "1                   ALDO               Men Driving Shoes   \n",
       "2                   Nike   LEBRON XVIII Basketball Shoes   \n",
       "3                   Nike    Men REACT INFINITY RUN Shoes   \n",
       "4                   Nike  Unisex COSMIC UNITY Basketball   \n",
       "..                   ...                             ...   \n",
       "95               Saint G      Women Leather Heeled Boots   \n",
       "96  Heel & Buckle London               Women Block Heels   \n",
       "97                 Ruosh          Men Leather Monk Shoes   \n",
       "98                  Geox            Men Leather Sneakers   \n",
       "99                 ASICS             Women Running Shoes   \n",
       "\n",
       "                     Shoe Price  \n",
       "0    Rs. 7721Rs. 10295(25% OFF)  \n",
       "1                     Rs. 11999  \n",
       "2   Rs. 11436Rs. 17595(35% OFF)  \n",
       "3                     Rs. 14495  \n",
       "4                     Rs. 13495  \n",
       "..                          ...  \n",
       "95    Rs. 6460Rs. 7600(15% OFF)  \n",
       "96    Rs. 6392Rs. 7990(20% OFF)  \n",
       "97                     Rs. 5490  \n",
       "98                    Rs. 10999  \n",
       "99    Rs. 6209Rs. 8999(31% OFF)  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating data frame for the data scraped\n",
    "shoe_data = pd.DataFrame({})\n",
    "shoe_data['Shoe Brand'] = brand\n",
    "shoe_data['Shoe Short Description'] = description\n",
    "shoe_data['Shoe Price'] = price\n",
    "shoe_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a760180c",
   "metadata": {},
   "source": [
    "# Q10: Go to webpage https://www.amazon.in/\n",
    "#### Enter “Laptop” in the search field and then click the search icon.\n",
    "#### Then set CPU Type filter to “Intel Core i7” and “Intel Core i9” \n",
    "#### After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "#### 1. title\n",
    "#### 2. Ratings\n",
    "#### 3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b235bd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating a new chrome window\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ae27e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opening the url of amazone.in\n",
    "url = \" https://www.amazon.in/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7eb78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching for laptops in search bar bar\n",
    "prod_search = driver.find_element_by_xpath('//input[@id=\"twotabsearchtextbox\"]')\n",
    "prod_search.send_keys('Laptop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3034021a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking on search button\n",
    "search_button = driver.find_element_by_xpath('//input[@id=\"nav-search-submit-button\"]')\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9f41490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting filter for cpu core i7\n",
    "cpu_filter_1 = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[1]/div/div[2]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[1]/li[26]/span/a/div/label/i')\n",
    "cpu_filter_1.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71650155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting filter for cpu core i9\n",
    "cpu_filter_2 = driver.find_element_by_xpath('/html/body/div[1]/div[2]/div[1]/div/div[2]/div/div[3]/span/div[1]/span/div/div/div[6]/ul[1]/li[28]/span/a/div/label/i')\n",
    "cpu_filter_2.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a47ae61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.amazon.in/Dell-i7-1165G7-Inspiron-5410-D560469WIN9S/dp/B095S9NJ4S/ref=sr_1_11?dchild=1&keywords=Laptop&qid=1627226511&refinements=p_n_feature_thirteen_browse-bin%3A12598163031%7C16757432031&rnid=12598141031&s=computers&sr=1-11#',\n",
       " 'https://www.amazon.in/gp/slredirect/picassoRedirect.html/ref=sspa_dk_detail_0?ie=UTF8&adId=A026612636M4EQCWPR91&qualifier=1627228681&id=2352504099980252&widgetName=sp_detail&url=%2Fdp%2FB08KJ9D2HD%2Fref%3Dsspa_dk_detail_0%3Fpsc%3D1%26pd_rd_i%3DB08KJ9D2HD%26pd_rd_w%3DxVsd4%26pf_rd_p%3D3d347ba3-873a-4950-a530-1b4d5938343e%26pd_rd_wg%3DjHwuj%26pf_rd_r%3DXY4NFZ6MWZY3QCEX1DXY%26pd_rd_r%3D5ee70230-2708-47e8-8a82-fc23f113e949',\n",
       " 'https://www.amazon.in/gp/slredirect/picassoRedirect.html/ref=sspa_dk_detail_0?ie=UTF8&adId=A026612636M4EQCWPR91&qualifier=1627228681&id=2352504099980252&widgetName=sp_detail&url=%2Fdp%2FB08KJ9D2HD%2Fref%3Dsspa_dk_detail_0%3Fpsc%3D1%26pd_rd_i%3DB08KJ9D2HD%26pd_rd_w%3DxVsd4%26pf_rd_p%3D3d347ba3-873a-4950-a530-1b4d5938343e%26pd_rd_wg%3DjHwuj%26pf_rd_r%3DXY4NFZ6MWZY3QCEX1DXY%26pd_rd_r%3D5ee70230-2708-47e8-8a82-fc23f113e949',\n",
       " 'https://www.amazon.in/gp/slredirect/picassoRedirect.html/ref=sspa_dk_detail_1?ie=UTF8&adId=A0970699C4ZQ2EHHDY6F&qualifier=1627228681&id=2352504099980252&widgetName=sp_detail&url=%2Fdp%2FB08MQ4CVQF%2Fref%3Dsspa_dk_detail_1%3Fpsc%3D1%26pd_rd_i%3DB08MQ4CVQF%26pd_rd_w%3DxVsd4%26pf_rd_p%3D3d347ba3-873a-4950-a530-1b4d5938343e%26pd_rd_wg%3DjHwuj%26pf_rd_r%3DXY4NFZ6MWZY3QCEX1DXY%26pd_rd_r%3D5ee70230-2708-47e8-8a82-fc23f113e949',\n",
       " 'https://www.amazon.in/gp/slredirect/picassoRedirect.html/ref=sspa_dk_detail_1?ie=UTF8&adId=A0970699C4ZQ2EHHDY6F&qualifier=1627228681&id=2352504099980252&widgetName=sp_detail&url=%2Fdp%2FB08MQ4CVQF%2Fref%3Dsspa_dk_detail_1%3Fpsc%3D1%26pd_rd_i%3DB08MQ4CVQF%26pd_rd_w%3DxVsd4%26pf_rd_p%3D3d347ba3-873a-4950-a530-1b4d5938343e%26pd_rd_wg%3DjHwuj%26pf_rd_r%3DXY4NFZ6MWZY3QCEX1DXY%26pd_rd_r%3D5ee70230-2708-47e8-8a82-fc23f113e949',\n",
       " 'https://www.amazon.in/gp/slredirect/picassoRedirect.html/ref=sspa_dk_detail_2?ie=UTF8&adId=A1008338T6BBJJCMT1VP&qualifier=1627228681&id=2352504099980252&widgetName=sp_detail&url=%2Fdp%2FB08CGKFW23%2Fref%3Dsspa_dk_detail_2%3Fpsc%3D1%26pd_rd_i%3DB08CGKFW23%26pd_rd_w%3DxVsd4%26pf_rd_p%3D3d347ba3-873a-4950-a530-1b4d5938343e%26pd_rd_wg%3DjHwuj%26pf_rd_r%3DXY4NFZ6MWZY3QCEX1DXY%26pd_rd_r%3D5ee70230-2708-47e8-8a82-fc23f113e949',\n",
       " 'https://www.amazon.in/gp/slredirect/picassoRedirect.html/ref=sspa_dk_detail_2?ie=UTF8&adId=A1008338T6BBJJCMT1VP&qualifier=1627228681&id=2352504099980252&widgetName=sp_detail&url=%2Fdp%2FB08CGKFW23%2Fref%3Dsspa_dk_detail_2%3Fpsc%3D1%26pd_rd_i%3DB08CGKFW23%26pd_rd_w%3DxVsd4%26pf_rd_p%3D3d347ba3-873a-4950-a530-1b4d5938343e%26pd_rd_wg%3DjHwuj%26pf_rd_r%3DXY4NFZ6MWZY3QCEX1DXY%26pd_rd_r%3D5ee70230-2708-47e8-8a82-fc23f113e949',\n",
       " 'https://www.amazon.in/gp/slredirect/picassoRedirect.html/ref=sspa_dk_detail_3?ie=UTF8&adId=A10081262K6KZOZ37JXYV&qualifier=1627228681&id=2352504099980252&widgetName=sp_detail&url=%2Fdp%2FB091FJ13Q5%2Fref%3Dsspa_dk_detail_3%3Fpsc%3D1%26pd_rd_i%3DB091FJ13Q5%26pd_rd_w%3DxVsd4%26pf_rd_p%3D3d347ba3-873a-4950-a530-1b4d5938343e%26pd_rd_wg%3DjHwuj%26pf_rd_r%3DXY4NFZ6MWZY3QCEX1DXY%26pd_rd_r%3D5ee70230-2708-47e8-8a82-fc23f113e949',\n",
       " 'https://www.amazon.in/gp/slredirect/picassoRedirect.html/ref=sspa_dk_detail_3?ie=UTF8&adId=A10081262K6KZOZ37JXYV&qualifier=1627228681&id=2352504099980252&widgetName=sp_detail&url=%2Fdp%2FB091FJ13Q5%2Fref%3Dsspa_dk_detail_3%3Fpsc%3D1%26pd_rd_i%3DB091FJ13Q5%26pd_rd_w%3DxVsd4%26pf_rd_p%3D3d347ba3-873a-4950-a530-1b4d5938343e%26pd_rd_wg%3DjHwuj%26pf_rd_r%3DXY4NFZ6MWZY3QCEX1DXY%26pd_rd_r%3D5ee70230-2708-47e8-8a82-fc23f113e949',\n",
       " 'https://www.amazon.in/gp/slredirect/picassoRedirect.html/ref=sspa_dk_detail_4?ie=UTF8&adId=A08698453270J5BF8YQDN&qualifier=1627228681&id=2352504099980252&widgetName=sp_detail&url=%2Fdp%2FB08SY2FZKY%2Fref%3Dsspa_dk_detail_4%3Fpsc%3D1%26pd_rd_i%3DB08SY2FZKY%26pd_rd_w%3DxVsd4%26pf_rd_p%3D3d347ba3-873a-4950-a530-1b4d5938343e%26pd_rd_wg%3DjHwuj%26pf_rd_r%3DXY4NFZ6MWZY3QCEX1DXY%26pd_rd_r%3D5ee70230-2708-47e8-8a82-fc23f113e949']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating empty lists for ratings and product url\n",
    "ratings = []\n",
    "prod_url = []\n",
    "\n",
    "# scraping data for each products url so that we can fetch ratings from each products page iteratively\n",
    "url1 = driver.find_elements_by_xpath('//a[@class=\"a-link-normal a-text-normal\"]') # getting web elements for url\n",
    "for i in url1[:10]:   # for loop for only 10 products webelement as we need only 10 laptops details\n",
    "    prod_url.append(i.get_attribute('href')) # appending 10 urls of products into empty list\n",
    "prod_url # printing urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1378bcd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prod_url)  # checking length of url list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3479935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping ratings data from each url iteratively\n",
    "for i in prod_url:      # for loop for opening each url iteratively to get rating data from each page iteratively\n",
    "    driver.get(i)       # opening the url\n",
    "    rtng = driver.find_elements_by_xpath('//span[@class=\"a-size-base a-nowrap\"]//span')  # web elements of rating data\n",
    "    for i in rtng:      # for loop for scraping rating data from full product page\n",
    "        ratings.append(i.text)   # appending the fetched data into empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a18161d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.3 out of 5',\n",
       " '3.2 out of 5',\n",
       " '3.2 out of 5',\n",
       " '5 out of 5',\n",
       " '5 out of 5',\n",
       " '4.5 out of 5',\n",
       " '4.5 out of 5',\n",
       " '4.1 out of 5',\n",
       " '4.1 out of 5',\n",
       " '4.9 out of 5']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings # printing scraped ratings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01a148e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ratings)  # checking the the length of ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8ee39432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list for titles\n",
    "title = []\n",
    "\n",
    "# scraping data for laptop titles\n",
    "L_title = driver.find_elements_by_xpath('//span[@class=\"a-size-medium a-color-base a-text-normal\"]')  # getting web elements of titles\n",
    "for i in L_title[:10]:    # for loop for scraping only 10 title from web elements\n",
    "    title.append(i.text)  # appending the scraped titles from web elements into empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8a3a18b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lenovo Yoga 7 11th Gen Intel Core i7-1165G7 14\" (35.56cm) FHD IPS 2-in-1 Touchscreen Laptop (16GB/512GB SSD/Windows 10/MS Office/Lenovo Digital Pen/Fingerprint Reader/Slate Grey/1.43Kg), 82BH004HIN',\n",
       " 'HP Envy 11th Gen Core i7 Processor 13.3-inch (33.78 cms) FHD Touchscreen Laptop (16GB/1TB SSD/Win 10/NVIDIA MX450 2GB/Natural Silver/1.3 kg), 13-ba1018TX',\n",
       " 'HP Pavilion (2021) Thin & Light 11th Gen Core i7 Laptop, 16 GB RAM, 1TB SSD, Iris Xe Graphics, 14\" (35.56cms) FHD Screen, Windows 10, MS Office, Backlit Keyboard (14-dv0058TU)',\n",
       " 'HP Pavilion x360 (2021) 14\" (35.56cms) FHD Touchscreen Laptop, 11th Gen Core i7, 8 GB RAM, 512GB SSD, 2-in-1 Convertible, Windows 10, MS Office, Finger Print Reader (14-dw1040TU)',\n",
       " 'Microsoft Laptop 2 1769 13.5-inch Laptop (Intel Core i7/8GB/256GB SSD/Windows 10 Home/Integrated Graphics), Platinum',\n",
       " 'Life Digital Laptop 15.6-inch (39.62 cms) (Intel Core i7, 8GB RAM, 512GB SSD, Windows 10), ZED AIR CX7',\n",
       " 'MSI GF65 Thin, Intel i7-10750H, 15.6\" FHD (39.6 cm) IPS-Level 144Hz Panel Laptop (16GB/512GB NVMe SSD/Windows 10 Home/Nvidia GTX1660 Ti 6GB GDDR6/Black/1.86Kg), 10SDR-1280IN',\n",
       " 'Dell G3 3500 Gaming Laptop 15.6\" (39.62cms) FHD 120 Hz (10th Gen Core i7-10750H/8GB/512GB SSD/Windows 10 Home Plus & MS Office/NVIDIA1650 Ti Graphics/Eclipse Black) D560260WIN9BE',\n",
       " 'HP Pavilion Gaming 10th Gen Intel Core i7 Processor 15.6-inch FHD Gaming Laptop (16GB/512GB SSD + 32GB Intel Optane/Windows 10/NVIDIA 1650Ti 4GB/Shadow Black), 15-dk1509TX',\n",
       " 'Mi Notebook Horizon Edition 14 Intel Core i7-10510U 10th Gen 14-inch (35.56 cms) Thin and Light Laptop(8GB/512GB SSD/Windows 10/Nvidia MX350 2GB Graphics/Grey/1.35Kg), XMA1904-AF+Webcam']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title  # printing titles to check if the data is fetched or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ae15b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty list for price\n",
    "price = []\n",
    "\n",
    "L_price = driver.find_elements_by_xpath('//span[@class=\"a-price-whole\"]')  # getting web elements of price data\n",
    "for i in L_price[:10]:      # for loop for only 10 price data from only 10 web elements\n",
    "    price.append(i.text)    # appending the scraped price data into empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e241ce44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['96,576',\n",
       " '1,09,990',\n",
       " '84,990',\n",
       " '81,002',\n",
       " '1,42,334',\n",
       " '46,322',\n",
       " '94,171',\n",
       " '86,390',\n",
       " '86,990',\n",
       " '59,999']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price  # printing the price data to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "58ec07f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(price)  # checking the lenth of price list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b675a143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Laptop Title</th>\n",
       "      <th>Laptop Ratings</th>\n",
       "      <th>Laptop Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lenovo Yoga 7 11th Gen Intel Core i7-1165G7 14...</td>\n",
       "      <td>4.3 out of 5</td>\n",
       "      <td>96,576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HP Envy 11th Gen Core i7 Processor 13.3-inch (...</td>\n",
       "      <td>3.2 out of 5</td>\n",
       "      <td>1,09,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HP Pavilion (2021) Thin &amp; Light 11th Gen Core ...</td>\n",
       "      <td>3.2 out of 5</td>\n",
       "      <td>84,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HP Pavilion x360 (2021) 14\" (35.56cms) FHD Tou...</td>\n",
       "      <td>5 out of 5</td>\n",
       "      <td>81,002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Microsoft Laptop 2 1769 13.5-inch Laptop (Inte...</td>\n",
       "      <td>5 out of 5</td>\n",
       "      <td>1,42,334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Life Digital Laptop 15.6-inch (39.62 cms) (Int...</td>\n",
       "      <td>4.5 out of 5</td>\n",
       "      <td>46,322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MSI GF65 Thin, Intel i7-10750H, 15.6\" FHD (39....</td>\n",
       "      <td>4.5 out of 5</td>\n",
       "      <td>94,171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Dell G3 3500 Gaming Laptop 15.6\" (39.62cms) FH...</td>\n",
       "      <td>4.1 out of 5</td>\n",
       "      <td>86,390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HP Pavilion Gaming 10th Gen Intel Core i7 Proc...</td>\n",
       "      <td>4.1 out of 5</td>\n",
       "      <td>86,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Mi Notebook Horizon Edition 14 Intel Core i7-1...</td>\n",
       "      <td>4.9 out of 5</td>\n",
       "      <td>59,999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Laptop Title Laptop Ratings  \\\n",
       "0  Lenovo Yoga 7 11th Gen Intel Core i7-1165G7 14...   4.3 out of 5   \n",
       "1  HP Envy 11th Gen Core i7 Processor 13.3-inch (...   3.2 out of 5   \n",
       "2  HP Pavilion (2021) Thin & Light 11th Gen Core ...   3.2 out of 5   \n",
       "3  HP Pavilion x360 (2021) 14\" (35.56cms) FHD Tou...     5 out of 5   \n",
       "4  Microsoft Laptop 2 1769 13.5-inch Laptop (Inte...     5 out of 5   \n",
       "5  Life Digital Laptop 15.6-inch (39.62 cms) (Int...   4.5 out of 5   \n",
       "6  MSI GF65 Thin, Intel i7-10750H, 15.6\" FHD (39....   4.5 out of 5   \n",
       "7  Dell G3 3500 Gaming Laptop 15.6\" (39.62cms) FH...   4.1 out of 5   \n",
       "8  HP Pavilion Gaming 10th Gen Intel Core i7 Proc...   4.1 out of 5   \n",
       "9  Mi Notebook Horizon Edition 14 Intel Core i7-1...   4.9 out of 5   \n",
       "\n",
       "  Laptop Price  \n",
       "0       96,576  \n",
       "1     1,09,990  \n",
       "2       84,990  \n",
       "3       81,002  \n",
       "4     1,42,334  \n",
       "5       46,322  \n",
       "6       94,171  \n",
       "7       86,390  \n",
       "8       86,990  \n",
       "9       59,999  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating the laptop data frame to organise all the data of laptops in data frame\n",
    "laptop_data = pd.DataFrame({})            # creating an empty data frame\n",
    "laptop_data['Laptop Title'] = title       # creating a title column in empty data frame and adding title data into title column\n",
    "laptop_data['Laptop Ratings'] = ratings   # creating rating column in data frame and adding rating data into rating column\n",
    "laptop_data['Laptop Price'] = price       # creating price column in data frame and adding price data into column\n",
    "# finally printing the data frame\n",
    "laptop_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad839da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
